{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25989e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여기가 __init__ 시작입니다\n",
      "여기가 __init__ 끝입니다\n",
      "inputs :  [[ 1. ]\n",
      " [ 0.5]\n",
      " [-1.5]]\n",
      "targets :  [[ 2. ]\n",
      " [ 1.5]\n",
      " [-1.5]]\n",
      "hidden_inputs :  [[-0.31703488]\n",
      " [-0.61026161]\n",
      " [ 0.10227644]]\n",
      "hidden_outputs :  [[0.42139854]\n",
      " [0.35199952]\n",
      " [0.52554684]]\n",
      "final_inputs :  [[ 0.51372225]\n",
      " [-0.05275391]\n",
      " [-0.3074151 ]]\n",
      "final_outputs :  [[0.62567865]\n",
      " [0.48681458]\n",
      " [0.42374581]]\n",
      "output_errors :  [[ 1.37432135]\n",
      " [ 1.01318542]\n",
      " [-1.92374581]]\n",
      "hidden_errors :  [[-2.39457917]\n",
      " [ 3.26932928]\n",
      " [ 2.09729791]]\n",
      "self.who :  [[-0.40844151  0.93151304  0.78723445]\n",
      " [ 0.24205953 -0.09063999 -0.15029197]\n",
      " [ 0.975136   -1.16969162 -0.73830661]]\n",
      "self.wih :  [[ 0.39884511 -0.16227091  0.83185846]\n",
      " [-0.01125218  0.30070248 -0.02243054]\n",
      " [-0.26249022  0.73216522 -0.36519168]]\n",
      "여기가 train 끝입니다\n",
      "질의 시작\n",
      "여기가 추론 하는 곳입니다 final_outputs print 합니다.\n",
      "[[0.71254968]\n",
      " [0.48012697]\n",
      " [0.30055315]]\n",
      "여기가 추론 하는 곳입니다 final_outputs return 합니다.\n",
      "Neural Network initialized.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import scipy.special\n",
    "\n",
    "class neuralNetwork:\n",
    "\n",
    "    #신경망 초기화하기 임력/은닉/출력 노드 수, 학습률 설정\n",
    "    def __init__(self,inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        # self.weights = []\n",
    "\n",
    "        print(\"여기가 __init__ 시작입니다\")\n",
    "\n",
    "        # 입력, 은닉, 출력 노드 수 param 으로 받은 arg 로 설정\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "\n",
    "        # 가중치 행렬 wih 와 who\n",
    "        # 배열내가중치는 Wij 로표기. 노드1에서 다음 계층의 노드 j로 연결됨을 의미\n",
    "        # 즉, w11 w12 행렬의 행과 열의 조합을 나타냄 by ***   \n",
    "        # w11 w21\n",
    "        # w12 w22 등\n",
    "\n",
    "        #입력에서 은닉으로 가중치 와 입력 은닉값 조합\n",
    "        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n",
    "        #은닉에서 출력으로 가중치 와 입력 은닉값 조합\n",
    "        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes)) \n",
    "\n",
    "        # Learning rate 학습율\n",
    "        self.lr = learningrate\n",
    "\n",
    "        #활성화 함수로는 시그모이드 함수를 이용\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "\n",
    "        print(\"여기가 __init__ 끝입니다\")\n",
    "        pass\n",
    "\n",
    "    #신경망 학습시키기\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        \n",
    "        # param 으로 받은 입력 리스트를 2차원 행렬로 변환\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "\n",
    "        print(\"inputs : \", inputs)\n",
    "        print(\"targets : \", targets)\n",
    "\n",
    "        # 은닉층으로 들어오는 신호를 계산\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # 은닉계층에서 나가는 신호를 계산\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        print(\"hidden_inputs : \", hidden_inputs)\n",
    "        print(\"hidden_outputs : \", hidden_outputs)\n",
    "\n",
    "\n",
    "        # 최종출력계층으로들어오는신호를계산 \n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # 최종출력계층에서 나가는 신호를 계산\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "        print(\"final_inputs : \", final_inputs)\n",
    "        print(\"final_outputs : \", final_outputs)\n",
    "\n",
    "\n",
    "        # 오차는 (실제값 - 예측값)입니다.\n",
    "        output_errors = targets - final_outputs\n",
    "\n",
    "        print(\"output_errors : \", output_errors)\n",
    "\n",
    "        # 역전파로 오차를 전달합니다.\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors)\n",
    "\n",
    "        print(\"hidden_errors : \", hidden_errors)\n",
    "\n",
    "        # 가중치의 업데이트\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        print(\"self.who : \", self.who)\n",
    "        print(\"self.wih : \", self.wih)\n",
    "        \n",
    "        print(\"여기가 train 끝입니다\")\n",
    "        pass\n",
    "\n",
    "    #신경망에 질의하기\n",
    "    def query(self, inputs_list):\n",
    "        print(\"질의 시작\")\n",
    "        # 입력 리스트를 2차원 행렬로 변환\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "\n",
    "        # 은닉층으로 들어오는 신호를 계산\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs) \n",
    "        # 위에서 간단한 코드로 모든 입력 값과 가중치를 연산함으로써 은닉계층의 각 노드로 들어오는 신호를 계산해 냅니다.\n",
    "\n",
    "        # 은닉계층에서 나가는 신호를 계산\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        # 최종출력계층으로들어오는신호를계산 \n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # 최종출력계층에서 나가는 신호를 계산\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        print(\"여기가 추론 하는 곳입니다 final_outputs print 합니다.\")\n",
    "        print(final_outputs)\n",
    "\n",
    "        #쥬피터에서 아래 리턴 동작안하는것 확인해야함~~\n",
    "        print(\"여기가 추론 하는 곳입니다 final_outputs return 합니다.\")\n",
    "        return final_outputs\n",
    "\n",
    "        # pass\n",
    "\n",
    "#    def add_layer(self, layer):\n",
    "#        self.weights.append(layer)\n",
    "    \n",
    "#    def forward(self, x):\n",
    "#        for weight in self.weights:\n",
    "#            x = self.activate(x @ weight)\n",
    "#        return x\n",
    "    \n",
    "#    def activate(self, x):\n",
    "#        return 1 / (1 + np.exp(-x))  # Sigmoid activation function    신경神經신경神經\n",
    "pass\n",
    "\n",
    "myNeural = neuralNetwork(3,3,3,0.3)\n",
    "myNeural.train([1.0, 0.5, -1.5], [2.0, 1.5, -1.5])\n",
    "myNeural.query([1.0, 0.5, -1.5])\n",
    "print(\"Neural Network initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cf758c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.14301639,  0.27939301],\n",
       "       [ 0.2918897 , -0.11566778],\n",
       "       [-0.3123712 , -0.28410014]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "a = numpy.zeros( [3,2] ) \n",
    "print(a)    \n",
    "numpy.random.rand(3,2) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6570d2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,18,18,18,126,136,175,26,166,255,247,127,0,0,0,0,0,0,0,0,0,0,0,0,30,36,94,154,170,253,253,253,253,253,225,172,253,242,195,64,0,0,0,0,0,0,0,0,0,0,0,49,238,253,253,253,253,253,253,253,253,251,93,82,82,56,39,0,0,0,0,0,0,0,0,0,0,0,0,18,219,253,253,253,253,253,198,182,247,241,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,80,156,107,253,253,205,11,0,43,154,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,1,154,253,90,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,139,253,190,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,190,253,70,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,35,241,225,160,108,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,240,253,253,119,25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,45,186,253,253,150,27,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,16,93,252,253,187,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,249,253,249,64,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,46,130,183,253,253,207,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,39,148,229,253,253,253,250,182,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,24,114,221,253,253,253,253,201,78,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,23,66,213,253,253,253,253,198,81,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,18,171,219,253,253,253,253,195,80,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,55,172,226,253,253,253,253,244,133,11,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,136,253,253,253,212,135,132,16,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "60000\n",
      "0\n",
      "60000\n",
      "******************************\n",
      "[0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.208      0.62729412 0.99223529 0.62729412 0.20411765\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.19635294 0.934\n",
      " 0.98835294 0.98835294 0.98835294 0.93011765 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.21964706 0.89129412 0.99223529 0.98835294 0.93788235\n",
      " 0.91458824 0.98835294 0.23129412 0.03329412 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.04882353 0.24294118 0.87964706\n",
      " 0.98835294 0.99223529 0.98835294 0.79423529 0.33611765 0.98835294\n",
      " 0.99223529 0.48364706 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.64282353 0.98835294 0.98835294 0.98835294 0.99223529\n",
      " 0.98835294 0.98835294 0.38270588 0.74376471 0.99223529 0.65835294\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.208      0.934\n",
      " 0.99223529 0.99223529 0.74764706 0.45258824 0.99223529 0.89517647\n",
      " 0.19247059 0.31670588 1.         0.66223529 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.19635294 0.934      0.98835294 0.98835294 0.70494118\n",
      " 0.05658824 0.30117647 0.47976471 0.09152941 0.01       0.01\n",
      " 0.99223529 0.95341176 0.20411765 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.15752941 0.65058824\n",
      " 0.99223529 0.91458824 0.81752941 0.33611765 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.99223529 0.98835294\n",
      " 0.65058824 0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.03717647 0.70105882 0.98835294 0.94176471 0.28564706\n",
      " 0.08376471 0.11870588 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.99223529 0.98835294 0.76705882 0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.23129412\n",
      " 0.98835294 0.98835294 0.25458824 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.99223529 0.98835294 0.76705882 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.77870588 0.99223529 0.74764706\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       1.         0.99223529\n",
      " 0.77094118 0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.30505882 0.96505882 0.98835294 0.44482353 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.99223529 0.98835294 0.58458824 0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.34       0.98835294\n",
      " 0.90294118 0.10705882 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.03717647 0.53411765\n",
      " 0.99223529 0.73211765 0.05658824 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.34       0.98835294 0.87576471 0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.03717647 0.51858824 0.98835294 0.88352941 0.28564706\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.34       0.98835294 0.57294118 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.19635294 0.65058824\n",
      " 0.98835294 0.68164706 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.34388235 0.99223529\n",
      " 0.88352941 0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.45258824 0.934      0.99223529 0.63894118 0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.34       0.98835294 0.97670588 0.57682353\n",
      " 0.19635294 0.12258824 0.34       0.70105882 0.88352941 0.99223529\n",
      " 0.87576471 0.65835294 0.22741176 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.34       0.98835294 0.98835294 0.98835294 0.89905882 0.84470588\n",
      " 0.98835294 0.98835294 0.98835294 0.77094118 0.51470588 0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.11870588 0.78258824\n",
      " 0.98835294 0.98835294 0.99223529 0.98835294 0.98835294 0.91458824\n",
      " 0.57294118 0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.10705882 0.50694118 0.98835294\n",
      " 0.99223529 0.98835294 0.55741176 0.15364706 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01      ]\n",
      "output nodes is1 0 (example)\n",
      "[0.99 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "import gzip\n",
    "%matplotlib inline\n",
    "\n",
    "# 훈련 데이터 로드하기 :: 책에 있는 소스로 gzip 적용 \n",
    "with gzip.open(\"mnist_train.gz\", 'r') as f:\n",
    "    training_data_list = [x.decode('utf8').strip() for x in f.readlines()]\n",
    "\n",
    "# data_file = open(\"mnist_train\", \"r\")\n",
    "# data_list = ata_file.readlines()\n",
    "# data_file.close()\n",
    "\n",
    "print(\"******************************\")\n",
    "print(training_data_list[0])\n",
    "print(training_data_list.__len__())\n",
    "print(training_data_list.count(0))\n",
    "print(len(training_data_list))\n",
    "print(\"******************************\")\n",
    "\n",
    "all_values = training_data_list[1].split(',')\n",
    "image_array = numpy.asarray(all_values[1:], dtype=float).reshape((28,28))\n",
    "matplotlib.pyplot.imshow(image_array, cmap='Greys', interpolation='None')  \n",
    "\n",
    "# convert pixel strings to float before dividing to avoid TypeError AI 가 수정\n",
    "pixels = numpy.asarray(all_values[1:], dtype=float)\n",
    "# print(pixels / 255.0)\n",
    "scaled_input = (pixels / 255.0 * 0.99) + 0.01 \n",
    "print(scaled_input)\n",
    "\n",
    "#output nodes is1 0 (example) \n",
    "onodes = 10\n",
    "targets = numpy.zeros (onodes) + 0.01 \n",
    "targets[int(all_values [0])] = 0.99\n",
    "print(\"output nodes is1 0 (example)\")\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output nodes is1 0 (example) \n",
    "onodes = 10\n",
    "targets = numpy.zeros (onodes) + 0.01 \n",
    "targets[int(all_values [0])] = 0.99"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
